{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978b0740-415c-4f58-a391-d00ded5f0127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (120, 256, 256, 3)\n",
      "Validation images shape: (40, 256, 256, 3)\n",
      "Training masks shape: (120, 256, 256, 1)\n",
      "Validation masks shape: (40, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define function to load and preprocess images and masks\n",
    "def load_data(train_image_dir, val_image_dir, train_mask_dir, val_mask_dir, image_size=(256, 256)):\n",
    "    train_images = []\n",
    "    val_images = []\n",
    "    train_masks = []\n",
    "    val_masks = []\n",
    "    \n",
    "    # Load and preprocess training images and masks\n",
    "    for image_file in os.listdir(train_image_dir):\n",
    "        image_path = os.path.join(train_image_dir, image_file)\n",
    "        mask_path = os.path.join(train_mask_dir, image_file.replace('.JPG', '_mask.JPG'))  # Assuming masks have same filenames with '_mask' suffix\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, image_size)\n",
    "        image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "        \n",
    "        train_images.append(image)\n",
    "        \n",
    "        # Load and preprocess mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, image_size)\n",
    "        mask = mask / 255.0  # Normalize pixel values to [0, 1]\n",
    "        mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "        \n",
    "        train_masks.append(mask)\n",
    "    \n",
    "    # Load and preprocess validation images and masks\n",
    "    for image_file in os.listdir(val_image_dir):\n",
    "        image_path = os.path.join(val_image_dir, image_file)\n",
    "        mask_path = os.path.join(val_mask_dir, image_file.replace('.JPG', '_mask.JPG'))  # Assuming masks have same filenames with '_mask' suffix\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, image_size)\n",
    "        image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "        \n",
    "        val_images.append(image)\n",
    "        \n",
    "        # Load and preprocess mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, image_size)\n",
    "        mask = mask / 255.0  # Normalize pixel values to [0, 1]\n",
    "        mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "        \n",
    "        val_masks.append(mask)\n",
    "    \n",
    "    return np.array(train_images), np.array(val_images), np.array(train_masks), np.array(val_masks)\n",
    "\n",
    "# Define directories containing images and masks\n",
    "train_image_dir = r'C:\\Users\\shanmuga\\OneDrive\\Documents\\mini project\\mini project\\UDD\\train\\train_image'\n",
    "val_image_dir = r'C:\\Users\\shanmuga\\OneDrive\\Documents\\mini project\\mini project\\UDD\\val\\val_image'\n",
    "train_mask_dir = r'C:\\Users\\shanmuga\\OneDrive\\Documents\\mini project\\mini project\\UDD\\train\\train_mask'\n",
    "val_mask_dir = r'C:\\Users\\shanmuga\\OneDrive\\Documents\\mini project\\mini project\\UDD\\val\\val_mask'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "train_images, val_images, train_masks, val_masks = load_data(train_image_dir, val_image_dir, train_mask_dir, val_mask_dir)\n",
    "\n",
    "# Display shapes of training and validation sets\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Training masks shape:\", train_masks.shape)\n",
    "print(\"Validation masks shape:\", val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711bfefc-2f7c-4f60-a4c8-4b8112c3f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 9s/step - accuracy: 0.3331 - f1_score: 0.2924 - loss: 1.1026 - mean_iou: 0.1750 - val_accuracy: 0.3335 - val_f1_score: 0.2506 - val_loss: 1.1007 - val_mean_iou: 0.1433\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.3333 - f1_score: 0.2729 - loss: 1.1004 - mean_iou: 0.1584 - val_accuracy: 0.3331 - val_f1_score: 0.3845 - val_loss: 1.0995 - val_mean_iou: 0.2380\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.3334 - f1_score: 0.3973 - loss: 1.0994 - mean_iou: 0.2480 - val_accuracy: 0.3328 - val_f1_score: 0.3742 - val_loss: 1.0990 - val_mean_iou: 0.2302\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7s/step - accuracy: 0.3335 - f1_score: 0.2620 - loss: 1.0989 - mean_iou: 0.1564 - val_accuracy: 0.3336 - val_f1_score: 0.1777 - val_loss: 1.0987 - val_mean_iou: 0.0975\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.3336 - f1_score: 0.2894 - loss: 1.0987 - mean_iou: 0.1738 - val_accuracy: 0.3334 - val_f1_score: 0.4460 - val_loss: 1.0987 - val_mean_iou: 0.2870\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 8s/step - accuracy: 0.3335 - f1_score: 0.4688 - loss: 1.0987 - mean_iou: 0.3065 - val_accuracy: 0.3329 - val_f1_score: 0.4976 - val_loss: 1.0987 - val_mean_iou: 0.3312\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.3335 - f1_score: 0.4984 - loss: 1.0986 - mean_iou: 0.3319 - val_accuracy: 0.3330 - val_f1_score: 0.4981 - val_loss: 1.0986 - val_mean_iou: 0.3317\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.3334 - f1_score: 0.4986 - loss: 1.0986 - mean_iou: 0.3321 - val_accuracy: 0.3330 - val_f1_score: 0.4981 - val_loss: 1.0986 - val_mean_iou: 0.3317\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 7s/step - accuracy: 0.3335 - f1_score: 0.4987 - loss: 1.0986 - mean_iou: 0.3321 - val_accuracy: 0.3337 - val_f1_score: 0.0587 - val_loss: 1.0986 - val_mean_iou: 0.0302\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7s/step - accuracy: 0.3334 - f1_score: 0.0580 - loss: 1.0986 - mean_iou: 0.0299 - val_accuracy: 0.3337 - val_f1_score: 0.0546 - val_loss: 1.0986 - val_mean_iou: 0.0281\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485ms/step - accuracy: 0.3338 - f1_score: 0.0547 - loss: 1.0986 - mean_iou: 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0986146926879883\n",
      "Validation Accuracy: 0.3337203860282898\n",
      "Mean IoU: 0.02806304767727852\n",
      "F1 Score: 0.05459398776292801\n",
      "Number of Trainable Parameters: 1941139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define custom IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.argmax(y_true, axis=-1) == 1, tf.math.argmax(y_pred, axis=-1) == 1), dtype=tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(tf.math.logical_or(tf.math.argmax(y_true, axis=-1) == 1, tf.math.argmax(y_pred, axis=-1) == 1), dtype=tf.float32))\n",
    "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "    return iou\n",
    "\n",
    "# Define custom F1 score metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.math.argmax(y_true, axis=-1) == 1, dtype=tf.float32)\n",
    "    y_pred = tf.cast(tf.math.argmax(y_pred, axis=-1) == 1, dtype=tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    \n",
    "    # Calculate TN\n",
    "    total_pixels = tf.cast(tf.size(y_true), dtype=tf.float32)\n",
    "    tn = total_pixels - (tp + fp + fn)\n",
    "    \n",
    "    precision = tp / ( tp + fp )\n",
    "    recall = tp / (tp + fn )\n",
    "    f1 = 2 * (precision * recall) / (precision + recall )\n",
    "    return f1\n",
    "\n",
    "# Define the architecture with Average Pooling\n",
    "def aerialsegnet(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(16, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.AveragePooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = layers.Conv2D(32, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.AveragePooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = layers.Conv2D(64, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.AveragePooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = layers.AveragePooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bottom layer\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Decoder...\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    up1 = layers.Conv2D(128, 2, activation='relu', padding='same')(up1)\n",
    "    merge1 = layers.concatenate([conv4, up1], axis=3)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge1)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "    up2 = layers.Conv2D(64, 2, activation='relu', padding='same')(up2)\n",
    "    merge2 = layers.concatenate([conv3, up2], axis=3)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge2)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up3 = layers.UpSampling2D(size=(2, 2))(conv7)\n",
    "    up3 = layers.Conv2D(32, 2, activation='relu', padding='same')(up3)\n",
    "    merge3 = layers.concatenate([conv2, up3], axis=3)\n",
    "    conv8 = layers.Conv2D(32, 3, activation='relu', padding='same')(merge3)\n",
    "    conv8 = layers.Conv2D(32, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up4 = layers.UpSampling2D(size=(2, 2))(conv8)\n",
    "    up4 = layers.Conv2D(16, 2, activation='relu', padding='same')(up4)\n",
    "    merge4 = layers.concatenate([conv1, up4], axis=3)\n",
    "    conv9 = layers.Conv2D(16, 3, activation='relu', padding='same')(merge4)\n",
    "    conv9 = layers.Conv2D(16, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    outputs = layers.Conv2D(3, 1, activation='softmax')(conv9)  # Assuming 3 classes\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "    # Define outputs\n",
    "    outputs = layers.Conv2D(3, 1, activation='softmax')(conv5)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define loss function (e.g., categorical cross-entropy) and optimizer\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Compile the model with custom metrics\n",
    "model = aerialsegnet(input_shape=(256, 256, 3))\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', mean_iou, f1_score])\n",
    "\n",
    "# Dummy data (replace with your actual dataset)\n",
    "train= np.random.randn(112, 256, 256, 3)\n",
    "train_mask= np.random.randint(0, 3, size=(112, 256,256))\n",
    "\n",
    "val_image = np.random.randn(40, 256,256, 3)\n",
    "test_mask = np.random.randint(0, 3, size=(40, 256, 256))\n",
    "\n",
    "# Convert target masks to one-hot encoded format\n",
    "train_mask = tf.keras.utils.to_categorical(train_mask, num_classes=3)\n",
    "test_mask = tf.keras.utils.to_categorical(test_mask, num_classes=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train, train_mask, epochs=10, validation_data=(val_image, test_mask))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy, mean_iou, f1= model.evaluate(val_image, test_mask)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "print(\"Mean IoU:\", mean_iou)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('lw-AerialSegNet_model_.h5')\n",
    "\n",
    "# Print the number of trainable and non-trainable parameters\n",
    "num_params = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(\"Number of Trainable Parameters:\", num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f5cf7-4383-4114-83d5-069c2079c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define function to preprocess input image\n",
    "def preprocess_image(image, target_size=(256, 256)):\n",
    "    # Resize image to match model input size\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    \n",
    "    # Normalize pixel values to be within [0, 1]\n",
    "    normalized_image = resized_image / 255.0\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('LW-AerialSegNet_model_.h5')\n",
    "\n",
    "# Define the color mappings for different classes\n",
    "class_colors = {\n",
    "    0: [255, 0, 0],  \n",
    "    1: [0, 0, 0],    \n",
    "    2: [0, 255, 0]   \n",
    "}\n",
    "\n",
    "def segment_image(image, model):\n",
    "    # Preprocess the input image\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    \n",
    "    # Perform segmentation using the U-Net model\n",
    "    segmented_image = model.predict(np.expand_dims(preprocessed_image, axis=0))[0]\n",
    "\n",
    "    # Convert predicted segmentation mask to class labels\n",
    "    segmented_image = np.argmax(segmented_image, axis=-1)\n",
    "    \n",
    "    # Resize the segmentation mask to match the original image size\n",
    "    resized_segmented_image = cv2.resize(segmented_image.astype(np.uint8), (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    # Convert class labels to color-coded segmentation mask\n",
    "    colored_mask = np.zeros_like(image)\n",
    "    for class_id, color in class_colors.items():\n",
    "        colored_mask[resized_segmented_image == class_id] = color\n",
    "\n",
    "    return colored_mask\n",
    "\n",
    "# Load input image\n",
    "image = cv2.imread(r'C:\\Users\\shanmuga\\OneDrive\\Documents\\mini project\\mini project\\UDD\\val\\val_mask\\DJI_0303_mask.JPG')\n",
    "\n",
    "# Segment the input image\n",
    "segmented_image = segment_image(image, model)\n",
    "\n",
    "# Display the input and segmented images\n",
    "cv2.namedWindow('Input Image', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Input Image', image)\n",
    "\n",
    "cv2.namedWindow('Segmented Image', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Segmented Image', segmented_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae35fb-3d1d-4100-84d6-ab30c07859cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
